{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hibames/pneumonia-detection-/blob/main/PD2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IUE-o64ULSy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjcnW9l2PCmy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "from tensorflow.keras.applications.efficientnet import EfficientNetB0, preprocess_input\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img, load_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZ4BZIy4Pi2X"
      },
      "outputs": [],
      "source": [
        "# Paths to data\n",
        "data_dir = '/content/drive/MyDrive/chest_xray'\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "val_dir = os.path.join(data_dir, 'val')\n",
        "test_dir = os.path.join(data_dir, 'test')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_dataset_path = '/content/drive/MyDrive/chest_xray/train/NORMAL'\n",
        "output_path = '/content/drive/MyDrive/chest_xray/train/NORMAL'  # Augmented images added here\n",
        "target_count = 2777  # Match PNEUMONIA training count\n"
      ],
      "metadata": {
        "id": "Vm6w3gELXlPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count existing images\n",
        "current_images = os.listdir(original_dataset_path)\n",
        "num_current = len(current_images)\n",
        "print(f\"Original NORMAL images: {num_current}\")\n"
      ],
      "metadata": {
        "id": "RPZZOo-7Y19y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDCo_ZguPSgS"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naMuebHgPcsG"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Load original images\n",
        "images = [f for f in os.listdir(original_dataset_path) if f.endswith('.jpeg') or f.endswith('.jpg') or f.endswith('.png')]\n",
        "num_current = len(images)\n",
        "print(f\"Original NORMAL images: {num_current}\")\n",
        "\n",
        "i = 0\n",
        "while len(os.listdir(output_path)) < target_count:\n",
        "    img_path = os.path.join(original_dataset_path, images[i % num_current])\n",
        "    img = load_img(img_path)\n",
        "    x = img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    for batch in datagen.flow(x, batch_size=1, save_to_dir=output_path, save_prefix='aug', save_format='jpeg'):\n",
        "        break  # Generate one image per loop iteration\n",
        "\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHhnPmSI7ut-"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.data.Dataset.from_generator(\n",
        "    lambda: train_data,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, 224, 224), dtype=tf.float32),  # Image shape and type\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.int32)  # Label shape and type\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "source_base = '/content/drive/MyDrive/chest_xray/train'\n",
        "classes = ['NORMAL', 'PNEUMONIA']\n",
        "\n",
        "train_base = 'split/train'\n",
        "val_base = 'split/val'\n",
        "test_base = 'split/test'\n",
        "\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15  # 15% for validation, remaining 15% for test\n",
        "\n",
        "for class_name in classes:\n",
        "    src_dir = os.path.join(source_base, class_name)\n",
        "    train_dir = os.path.join(train_base, class_name)\n",
        "    val_dir = os.path.join(val_base, class_name)\n",
        "    test_dir = os.path.join(test_base, class_name)\n",
        "\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    all_files = [f for f in os.listdir(src_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "    random.shuffle(all_files)\n",
        "\n",
        "    total = len(all_files)\n",
        "    train_end = int(total * train_ratio)\n",
        "    val_end = int(total * (train_ratio + val_ratio))\n",
        "\n",
        "    train_files = all_files[:train_end]\n",
        "    val_files = all_files[train_end:val_end]\n",
        "    test_files = all_files[val_end:]\n",
        "\n",
        "    for f in train_files:\n",
        "        shutil.copy(os.path.join(src_dir, f), os.path.join(train_dir, f))\n",
        "    for f in val_files:\n",
        "        shutil.copy(os.path.join(src_dir, f), os.path.join(val_dir, f))\n",
        "    for f in test_files:\n",
        "        shutil.copy(os.path.join(src_dir, f), os.path.join(test_dir, f))\n",
        "\n",
        "    print(f\"{class_name} - Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}\")\n"
      ],
      "metadata": {
        "id": "4oGV4XPMhUF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255)  # You might need other preprocessing steps\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    'split/train',  # Path to your training data\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'  # Adjust if you have more than 2 classes\n",
        ")\n",
        "\n",
        "val_data = val_datagen.flow_from_directory(\n",
        "    'split/val',  # Path to your validation data\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(\n",
        "    'split/test',  # Path to your test data\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    shuffle=False  # Important for consistent evaluation\n",
        ")"
      ],
      "metadata": {
        "id": "8FD027rRjTKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEYzAG-bQMPV"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Load pre-trained VGG16\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
        "\n",
        "def extract_features(directory):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for label in ['NORMAL', 'PNEUMONIA']:\n",
        "        folder = os.path.join(directory, label)\n",
        "        for filename in os.listdir(folder):\n",
        "            path = os.path.join(folder, filename)\n",
        "            img = load_img(path, target_size=(224, 224))\n",
        "            img_array = img_to_array(img)\n",
        "            img_array = preprocess_input(img_array)\n",
        "            img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "            feature = vgg_model.predict(img_array, verbose=0)\n",
        "            features.append(feature.flatten())\n",
        "            labels.append(0 if label == 'NORMAL' else 1)\n",
        "\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Extract features from train and test\n",
        "X_train, y_train = extract_features('split/train')\n",
        "X_test, y_test = extract_features('split/test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkF61b10QQK4"
      },
      "outputs": [],
      "source": [
        "# --- Extract Features ---\n",
        "X_train_feat, y_train = extract_features('split/train')\n",
        "X_val_feat, y_val = extract_features('split/val')\n",
        "X_test_feat, y_test = extract_features('split/test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCqoBkEZQU_g"
      },
      "outputs": [],
      "source": [
        "# -scaler = StandardScaler()\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_feat)\n",
        "X_test_scaled = scaler.transform(X_test_feat)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSucmCFGQZrh",
        "outputId": "a6e14d2f-652c-4d03-fb84-f282f83a1910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected feature indices: [  0  23  54  85 102 129 139 159 161 164 174 177 213 216 227 229 321 340\n",
            " 375 394]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Apply SelectKBest to extract top k features\n",
        "k = 20  # You can adjust this number based on your dataset\n",
        "selector = SelectKBest(score_func=f_classif, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "# Optional: Check selected feature indices\n",
        "selected_features = selector.get_support(indices=True)\n",
        "print(\"Selected feature indices:\", selected_features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, class_weight='balanced')\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"Random Forest:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1]))\n",
        "\n",
        "# XGBoost\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "print(\"\\nXGBoost:\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vClX-SnE1sR4",
        "outputId": "46f37cff-a3ee-42cc-b754-fc0df0ce17b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.95      0.95       417\n",
            "           1       0.97      0.96      0.97       591\n",
            "\n",
            "    accuracy                           0.96      1008\n",
            "   macro avg       0.96      0.96      0.96      1008\n",
            "weighted avg       0.96      0.96      0.96      1008\n",
            "\n",
            "Confusion Matrix:\n",
            " [[398  19]\n",
            " [ 21 570]]\n",
            "ROC AUC: 0.9882672542169312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [00:04:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XGBoost:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97       417\n",
            "           1       0.98      0.97      0.98       591\n",
            "\n",
            "    accuracy                           0.98      1008\n",
            "   macro avg       0.97      0.98      0.97      1008\n",
            "weighted avg       0.98      0.98      0.98      1008\n",
            "\n",
            "Confusion Matrix:\n",
            " [[407  10]\n",
            " [ 15 576]]\n",
            "ROC AUC: 0.9950171842221655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    # Clip predictions to avoid log(0)\n",
        "    epsilon = 1e-15\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "    # Compute BCE loss\n",
        "    bce_loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return bce_loss\n",
        "\n",
        "# Compute BCE Loss for XGBoost\n",
        "y_proba_xgb = xgb.predict_proba(X_test_selected)[:, 1]\n",
        "xgb_bce_loss_manual = binary_cross_entropy(y_test, y_proba_xgb)\n",
        "print(\"Manual XGBoost Binary Cross-Entropy Loss (BCE Loss):\", xgb_bce_loss_manual)\n",
        "\n",
        "# Compute BCE Loss for Random Forest\n",
        "y_proba_rf = rf.predict_proba(X_test_selected)[:, 1]\n",
        "rf_bce_loss_manual = binary_cross_entropy(y_test, y_proba_rf)\n",
        "print(\"Manual Random Forest Binary Cross-Entropy Loss (BCE Loss):\", rf_bce_loss_manual)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "dvTqnX8ayr2U",
        "outputId": "565c7fa6-4508-4057-84e0-08fdb48fd132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Feature shape mismatch, expected: 512, got 20",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-2ff69e5b7b96>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Compute BCE Loss for XGBoost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0my_proba_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mxgb_bce_loss_manual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_proba_xgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Manual XGBoost Binary Cross-Entropy Loss (BCE Loss):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_bce_loss_manual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1710\u001b[0m             \u001b[0mclass_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_predt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mclass_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m         class_probs = super().predict(\n\u001b[0m\u001b[1;32m   1713\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m             \u001b[0mvalidate_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_use_inplace_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m                     predts = self.get_booster().inplace_predict(\n\u001b[0m\u001b[1;32m   1249\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                         \u001b[0miteration_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2522\u001b[0m                 )\n\u001b[1;32m   2523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   2525\u001b[0m                     \u001b[0;34mf\"Feature shape mismatch, expected: {self.num_features()}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m                     \u001b[0;34mf\"got {data.shape[1]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Feature shape mismatch, expected: 512, got 20"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGcBXpnCeS7l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bfeee43-6c3e-4186-82fe-d5ec26820b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m147/147\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 481ms/step - accuracy: 0.5629 - loss: 0.7183 - val_accuracy: 0.7192 - val_loss: 0.5587 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m 70/147\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 391ms/step - accuracy: 0.6809 - loss: 0.5897"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Load VGG19 pretrained model (without the classification head)\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # Freeze convolutional base\n",
        "\n",
        "# Add custom top layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1, activation='sigmoid')(x)  # For binary classification\n",
        "\n",
        "# Build the final model\n",
        "cnn_model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "# Compile the model\n",
        "cnn_model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=3, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(train_data, validation_data=val_data, epochs=10, callbacks=callbacks, verbose=1)\n",
        "\n",
        "# Predict\n",
        "y_pred_cnn_proba = cnn_model.predict(test_data).ravel()\n",
        "y_pred_cnn = (y_pred_cnn_proba > 0.5).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5XUvVhgeW2Y"
      },
      "outputs": [],
      "source": [
        "# --- EVALUATION FUNCTION ---\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score # Importing necessary functions\n",
        "def evaluate_model(name, y_true, y_pred, y_proba):\n",
        "    print(f\"\\n{name} Evaluation\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    auc = roc_auc_score(y_true, y_proba)\n",
        "    print(f\"ROC AUC: {auc:.4f}\")\n",
        "    return {\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Precision': precision_score(y_true, y_pred),\n",
        "        'Recall': recall_score(y_true, y_pred),\n",
        "        'F1-Score': f1_score(y_true, y_pred),\n",
        "        'ROC AUC': auc\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uotOBUXGeb16"
      },
      "outputs": [],
      "source": [
        "# --- COMPARE MODELS ---\n",
        "y_pred_rf_proba = rf.predict_proba(X_test_selected)[:, 1]\n",
        "y_pred_xgb_proba = xgb.predict_proba(X_test_selected)[:, 1]\n",
        "\n",
        "y_pred_xgb = xgb.predict(X_test_selected)\n",
        "\n",
        "results = []\n",
        "results.append(evaluate_model(\"Random Forest\", y_test, y_pred_rf, y_pred_rf_proba))\n",
        "results.append(evaluate_model(\"XGBoost\", y_test, y_pred_xgb, y_pred_xgb_proba))\n",
        "results.append(evaluate_model(\"CNN\", y_test, y_pred_cnn, y_pred_cnn_proba))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIbGSlGUvn9q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9skZireeiJX"
      },
      "outputs": [],
      "source": [
        "# --- VISUALIZE COMPARISON ---\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.set_index('Model', inplace=True)\n",
        "results_df.plot(kind='bar', figsize=(10,6), ylim=(0,1), title='Model Performance Comparison', ylabel='Score')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL3KlG31zTqW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syXD8E12fT-n"
      },
      "outputs": [],
      "source": [
        "# --- PLOT ROC CURVES ---\n",
        "from sklearn.metrics import roc_curve, roc_auc_score # Import roc_curve\n",
        "def plot_roc_curve(y_true, y_proba, label):\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_proba)\n",
        "    plt.plot(fpr, tpr, label=f'{label} (AUC = {roc_auc_score(y_true, y_proba):.2f})')\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plot_roc_curve(y_test, y_pred_rf_proba, 'Random Forest')\n",
        "plot_roc_curve(y_test, y_pred_xgb_proba, 'XGBoost')\n",
        "plot_roc_curve(y_test, y_pred_cnn_proba, 'CNN')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "authorship_tag": "ABX9TyNZDoxHxRHFFwuReM4U52qv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}